{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e37279-fb54-4aea-b49d-ae4dad6c009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dgl\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import chain\n",
    "from dgl.data import DGLDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8abb2-0eef-475f-84b1-ec529aaad812",
   "metadata": {},
   "source": [
    "# Step 1: Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f69566-6710-4480-bfa7-751eb2fd52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CB12Dataset(DGLDataset):\n",
    "    \"\"\"\n",
    "    CareerBuilder12 dataset for node classification\n",
    "    \n",
    "    \n",
    "    Dataset statistics:\n",
    "    \n",
    "    - Nodes: \n",
    "    - Node features: \n",
    "    - Edges: \n",
    "    - Edge Weights:\n",
    "    - Number of Classes: \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of node classes\n",
    "    data : list\n",
    "        A list of :class:`dgl.DGLGraph` objects\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CB12Dataset, self).__init__(name='cb12')\n",
    "    \n",
    "    def process(self):\n",
    "        nodes_data = pd.read_csv(\"../data/cb12/graph/titles.csv\", \"\\t\")\n",
    "        edges_data = pd.read_csv(\"../data/cb12/graph/title_title_transition_MinorGroup200.csv\", \"\\t\")\n",
    "        edges_src = torch.from_numpy(edges_data['Src'].to_numpy())\n",
    "        edges_dst = torch.from_numpy(edges_data['Dst'].to_numpy())\n",
    "        \n",
    "        # Node feature\n",
    "        all_tokens = chain.from_iterable([eval(item) for item in nodes_data['JobTitle_tokens_idx']])\n",
    "        vocab_size = len(set(all_tokens))\n",
    "        node_features = []\n",
    "        for node in nodes_data['JobTitle_tokens_idx'].tolist():\n",
    "            feature = [0 for _ in range(vocab_size)]\n",
    "            for i in eval(node):\n",
    "                feature[i] = 1\n",
    "            node_features.append(feature)\n",
    "        \n",
    "        node_features = torch.from_numpy(np.array(node_features)).float()\n",
    "        edge_features = torch.from_numpy(edges_data['Weight'].to_numpy())\n",
    "        \n",
    "        \n",
    "        self.all_labels = nodes_data[\"MajorGroup\"].tolist()\n",
    "        label_to_id = {label: idx for idx, label in enumerate(set(self.all_labels))}\n",
    "        print(dict(enumerate(nodes_data['MajorGroup'].astype('category').cat.categories)))\n",
    "        node_labels = torch.from_numpy(nodes_data['MajorGroup'].astype('category').cat.codes.to_numpy()).int()\n",
    "        \n",
    "        \n",
    "        g = dgl.graph((edges_src, edges_dst), num_nodes=nodes_data.shape[0])\n",
    "        #self.graph = dgl.add_self_loop(g)\n",
    "        self.graph = g\n",
    "        \n",
    "        \n",
    "        self.graph.ndata['feature'] = node_features\n",
    "        self.graph.ndata['label'] = node_labels\n",
    "        self.graph.edata['weight'] = edge_features \n",
    "        \n",
    "        \n",
    "        n_nodes = nodes_data.shape[0]\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        self.graph.ndata['train_mask'] = train_mask\n",
    "        self.graph.ndata['val_mask'] = val_mask\n",
    "        self.graph.ndata['test_mask'] = test_mask\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get graph object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Item index\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`dgl.DGLGraph`\n",
    "            graph structure and labels.\n",
    "            - ``ndata['label']``: ground truth labelsv\n",
    "        \"\"\"\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        return self.graph\n",
    "    \n",
    "    def __len__(self):\n",
    "        r\"\"\"The number of graphs in the dataset.\"\"\"\n",
    "        return 1   \n",
    "    \n",
    "    \n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "   \n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Number of classes.\"\"\"\n",
    "        print(\"Number of classes: {}\".format(len(set(self.all_labels))))\n",
    "        return len(set(self.all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cb2f3-30a4-4b5f-8642-4a126b776c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cb12 = CB12Dataset()\n",
    "graph_cb12 = dataset_cb12[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7865706-0334-4431-b367-6c7b725078e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_cb12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b8c76-739f-40d2-b553-b9ddb63f096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = graph_cb12.ndata['label']\n",
    "\n",
    "train_mask = graph_cb12.ndata['train_mask']\n",
    "val_mask = graph_cb12.ndata['val_mask']\n",
    "test_mask = graph_cb12.ndata['test_mask']\n",
    "    \n",
    "    \n",
    "train_labels = labels[train_mask]\n",
    "val_labels = labels[val_mask]\n",
    "test_labels = labels[test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea1b0b-1ea2-438c-ab98-ac2b0fe4d28c",
   "metadata": {},
   "source": [
    "# Step 2: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e0806-b6b3-4240-9a6a-2dcb0dc04916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfac88d-3b71-4208-9641-3323f2c72e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits, labels):\n",
    "    _, indices = torch.max(logits, dim=1)\n",
    "    preds = indices.long().cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    micro_f1 = f1_score(labels, preds, average='micro')\n",
    "    weighted_f1 = f1_score(labels, preds, average='weighted')\n",
    "    return acc, macro_f1, micro_f1, weighted_f1\n",
    "\n",
    "\n",
    "def evaluate(model, g, features, labels, mask, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "    loss = loss_fn(logits, labels.long())\n",
    "    acc, macro_f1, micro_f1, weighted_f1 = compute_metrics(logits, labels)\n",
    "    return loss, acc, macro_f1, micro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55e15b-69e6-4bb3-a593-7e8d5347c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        dt = datetime.datetime.now()\n",
    "        self.filename = 'early_stop_{}_{:02d}-{:02d}-{:02d}.pth'.format(dt.date(), dt.hour, dt.minute, dt.second)\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def save_checkpoint(self, model, model_name):\n",
    "        \"\"\"\n",
    "        Save model when validation loss decrease\n",
    "        \"\"\"\n",
    "        dirs = os.path.join('../checkpoints/' + model_name)\n",
    "        if not os.path.exists(dirs):\n",
    "            os.makedirs(dirs)\n",
    "        torch.save(model.state_dict(), os.path.join(dirs + '/' +  self.filename))\n",
    "    \n",
    "    def load_checkpoint(self, model, model_name):\n",
    "        \"\"\"\n",
    "        Load the latest checkpoint\n",
    "        \"\"\"\n",
    "        model.load_state_dict(torch.load(os.path.join('../checkpoints/' + model_name + '/' + self.filename)))   \n",
    "    \n",
    "    def step(self, model, model_name, loss, acc):\n",
    "        score = acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_loss = loss\n",
    "            self.save_checkpoint(model, model_name)\n",
    "            \n",
    "        elif (loss > self.best_loss) and (acc < self.best_score):\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        else:\n",
    "            if (loss <= self.best_loss) and (acc >= self.best_score):\n",
    "                self.save_checkpoint(model, model_name)\n",
    "                \n",
    "            self.best_score = np.max((acc, self.best_score))\n",
    "            self.best_loss = np.min((loss, self.best_loss))\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def train(g, model, model_name, lr, weight_decay, epoch):\n",
    "    stopper = EarlyStopping(patience=100)\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    best_val_macro_f1 = 0\n",
    "    best_test_macro_f1 = 0\n",
    "\n",
    "    features = g.ndata['feature']\n",
    "    labels = g.ndata['label']\n",
    "    \n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    \n",
    "    \n",
    "    list_train_loss = []\n",
    "    \n",
    "    list_train_acc = []\n",
    "    list_val_acc = []\n",
    "    list_test_acc = []\n",
    "    \n",
    "    list_train_macro_f1 = []\n",
    "    list_val_macro_f1  = []\n",
    "    list_test_macro_f1  = []\n",
    "    \n",
    "    list_train_micro_f1 = []\n",
    "    list_val_micro_f1  = []\n",
    "    list_test_micro_f1  = []\n",
    "    \n",
    "    list_train_weighted_f1 = []\n",
    "    list_val_weighted_f1  = []\n",
    "    list_test_weighted_f1  = []\n",
    "    \n",
    "    \n",
    "    for e in range(epoch):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask].long())\n",
    "        list_train_loss.append(loss.detach().numpy())\n",
    "        \n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc, train_macro_f1, train_micro_f1, train_weighted_f1 = compute_metrics(logits[train_mask], labels[train_mask])\n",
    "        list_train_acc.append(train_acc)\n",
    "        list_train_macro_f1.append(train_macro_f1)\n",
    "        list_train_micro_f1.append(train_micro_f1)\n",
    "        list_train_weighted_f1.append(train_weighted_f1)\n",
    "    \n",
    "        \n",
    "        val_loss, val_acc, val_macro_f1, val_micro_f1, val_weighted_f1 = evaluate(model, g, features, labels, val_mask, loss_fcn)\n",
    "        list_val_acc.append(val_acc)\n",
    "        list_val_macro_f1.append(val_macro_f1)\n",
    "        list_val_micro_f1.append(val_micro_f1)\n",
    "        list_val_weighted_f1.append(val_weighted_f1)\n",
    "        \n",
    "        if stopper.step(model, model_name, val_loss, val_acc):\n",
    "            break\n",
    "        \n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "\n",
    "        if e % 100 == 0:\n",
    "            print('In epoch {}, loss: {:.4f}'.format(e, loss))\n",
    "            print('train acc: {:.4f}, val acc: {:.4f} (best {:.4f})'.format(train_acc, val_acc, best_val_acc))\n",
    "            print('train macro_f1: {:.4f}, val macro_f1: {:.4f}'.format(train_macro_f1, val_macro_f1))\n",
    "            print('train micro_f1: {:.4f}, val micro_f1: {:.4f}'.format(train_micro_f1, val_micro_f1))\n",
    "            print('train weighted_f1: {:.4f}, val weighted_f1: {:.4f}'.format(train_weighted_f1, val_weighted_f1))\n",
    "            print(\"-----------------------------\")\n",
    "    \n",
    "    stopper.load_checkpoint(model, model_name)\n",
    "    test_loss, test_acc, test_macro_f1, test_micro_f1, test_weighted_f1 = evaluate(model, g, features, labels, test_mask, loss_fcn)\n",
    "    print('test acc: {:.4f}, test macro_f1: {:.4f}, test micro_f1: {:.4f}, test weighted_f1: {:.4f}'.format(test_acc, test_macro_f1, test_micro_f1, test_weighted_f1))\n",
    "            \n",
    "    \n",
    "    \n",
    "    results =  pd.DataFrame({'loss': list_train_loss, \n",
    "                'train_acc': list_train_acc, \n",
    "                'val_acc':list_val_acc,\n",
    "                'train_macro_f1': list_train_macro_f1,\n",
    "                'val_macro_f1': list_val_macro_f1,\n",
    "                'train_micro_f1': list_train_micro_f1,\n",
    "                'val_micro_f1': list_val_micro_f1,\n",
    "                'train_weighted_f1': list_train_weighted_f1,\n",
    "                'val_weighted_f1': list_val_weighted_f1,\n",
    "               })\n",
    "    \n",
    "    dirs = os.path.join('results/' + model_name)\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "            \n",
    "    f_out = open(os.path.join(dirs + '/' + 'lr' + str(lr) +'.pkl'), 'wb')\n",
    "    pickle.dump(results, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c77c89-b620-4c46-9ee4-6950b1eac837",
   "metadata": {},
   "source": [
    "### GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d728443-125b-4359-87e2-e202c45269ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502d9b2-ecc7-43c9-9e58-f44209c8827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, activation, dropout):\n",
    "        \"\"\"\n",
    "        :param in_feats[int]: dimension of input features\n",
    "        :param n_hidden[int]: number of hidden units\n",
    "        :param n_classes[int]: number of classes\n",
    "        :param n_layers[int]: number of gcn layers\n",
    "        :param activation[str]: \n",
    "        :param dropout[float]: \n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.layers = nn.ModuleList()\n",
    "        # Input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=self.activation))\n",
    "        # Hidden layer\n",
    "        for i in range(n_layers-1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=self.activation))\n",
    "        # Output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i !=0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "        return h\n",
    "    \n",
    "    def embedding(self, g, x, nodes=None):\n",
    "        \"\"\"\n",
    "        Returns the embeddings of the input nodes\n",
    "        Parameters\n",
    "        ----------\n",
    "        nodes: Tensor, optional\n",
    "            Input nodes, if set `None`, will return all the node embedding.\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Node embedding.\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = layer(g, h)\n",
    "                h = self.activation(h)\n",
    "        \n",
    "        dirs = os.path.join('embs/' + model_name)\n",
    "        if not os.path.exists(dirs):\n",
    "            os.makedirs(dirs)\n",
    "        \n",
    "        f_out = open(dirs + '/' + 'lr' + str(lr) +'.pkl', 'wb')\n",
    "        pickle.dump(h, f_out)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a7e35-4652-46a8-a297-852677ac5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GCN = GCN(\n",
    "            in_feats=graph_cb12.ndata['feature'].shape[1],\n",
    "            n_hidden=128,\n",
    "            n_classes=dataset_cb12.num_classes,\n",
    "            n_layers=1,\n",
    "            activation=F.relu,\n",
    "            dropout=0.0\n",
    "           )\n",
    "train(graph_cb12, model_GCN, 'GCN', lr=0.001, weight_decay=0.0005, epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54e4bc-ff2c-4e23-8b53-e450c03ec744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee4e2c-80d3-491a-b80c-8de900d94716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
