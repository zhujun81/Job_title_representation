{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211e47d-b0e7-4b6b-a202-d3f501a87b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dgl\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import chain\n",
    "from dgl.data import DGLDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ace901-c7d7-4e20-ace9-249ca53a11b5",
   "metadata": {},
   "source": [
    "# Step 1: Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1acc11-9e90-4d10-a2fb-11df28fa5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CB12Dataset(DGLDataset):\n",
    "    \"\"\"\n",
    "    CB12 resume dataset for node classification\n",
    "    \n",
    "    \n",
    "    Dataset statistics:\n",
    "    \n",
    "    - Nodes: \n",
    "    - Node features: \n",
    "    - Edges: \n",
    "    - Edge Weights:\n",
    "    - Number of Classes: \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of node classes\n",
    "    data : list\n",
    "        A list of :class:`dgl.DGLGraph` objects\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CB12Dataset, self).__init__(name='CB12')\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        # Node data\n",
    "        \n",
    "        ## title\n",
    "        title_ids = []\n",
    "        title_names = []\n",
    "        f_title = open(\"../data/cb12/graph/id_title.txt\", encoding=\"ISO-8859-1\")\n",
    "        while True:\n",
    "            id_title = f_title.readline()\n",
    "            if not id_title:\n",
    "                break\n",
    "                \n",
    "            id_title = id_title.strip().split('\\t')\n",
    "            title_identity = int(id_title[0])\n",
    "            title_ids.append(title_identity)\n",
    "            title_name = id_title[1]\n",
    "            title_names.append(title_name)\n",
    "        \n",
    "        f_title.close()\n",
    "        print(\"Unique titles: {}\".format(len(title_ids)))\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        ## tag\n",
    "        tag_ids = []\n",
    "        tag_names = []\n",
    "        f_tag = open(\"../data/cb12/graph/id_tag.txt\", encoding=\"ISO-8859-1\")\n",
    "        while True:\n",
    "            id_tag = f_tag.readline()\n",
    "            if not id_tag:\n",
    "                break\n",
    "            \n",
    "            id_tag = id_tag.strip().split('\\t')\n",
    "            tag_identity = int(id_tag[0])\n",
    "            tag_ids.append(tag_identity)\n",
    "            tag_name = id_tag[1]\n",
    "            tag_names.append(tag_name)\n",
    "        f_tag.close()\n",
    "        print(\"Unique tags: {}\".format(len(tag_ids)))\n",
    "        print('\\n')\n",
    "        \n",
    "        title_ids_invmap = {x: i for i, x in enumerate(title_ids)}\n",
    "        tag_ids_invmap = {x: i for i, x in enumerate(tag_ids)}\n",
    "        \n",
    "      \n",
    "        \n",
    "        # Title feature\n",
    "        token_idx = []\n",
    "        f_title_feature = open(\"../data/cb12/graph/title_feature.txt\", \"r\")\n",
    "        for title_feature in f_title_feature:\n",
    "            title_feature = title_feature.split('\\t')\n",
    "            identity = int(title_feature[0])\n",
    "            feature = title_feature[1]\n",
    "            token_idx.append(feature)\n",
    "        all_tokens = chain.from_iterable([eval(item) for item in token_idx])\n",
    "        vocab_size = len(set(all_tokens))\n",
    "        print('Vocab size: ', vocab_size)\n",
    "        f_title_feature.close()\n",
    "        \n",
    "        node_features = [[0 for _ in range(vocab_size)] for _ in range(len(title_ids_invmap))]\n",
    "        f_title_feature = open(\"../data/cb12/graph/title_feature.txt\", \"r\")\n",
    "        for title_feature in f_title_feature:\n",
    "            title_feature = title_feature.split('\\t')\n",
    "            identity = int(title_feature[0])\n",
    "            feature = title_feature[1]\n",
    "            title_id = title_ids_invmap[identity]\n",
    "            for i in eval(feature):\n",
    "                node_features[title_id][i] = 1\n",
    "        node_features = torch.from_numpy(np.array(node_features)).float()\n",
    "        print(node_features.shape)\n",
    "        f_title_feature.close()\n",
    "        \n",
    "        # Title label \n",
    "        all_labels = []\n",
    "        f_title_label = open(\"../data/cb12/graph/title_label.txt\", \"r\")\n",
    "        for title_label in f_title_label:\n",
    "            title_label = title_label.split('\\t')\n",
    "            identity = int(title_label[0])\n",
    "            # MinorGroup, MajorGroup\n",
    "            label = title_label[2]\n",
    "            all_labels.append(label)\n",
    "        self.all_labels = all_labels\n",
    "        label_to_id = {label: idx for idx, label in enumerate(set(self.all_labels))}\n",
    "        print('Number of labels: ', len(label_to_id))\n",
    "        f_title_label.close()\n",
    "        \n",
    "        node_labels = np.zeros(len(title_ids_invmap), dtype=np.int64)\n",
    "        f_title_label = open(\"../data/cb12/graph/title_label.txt\", \"r\")\n",
    "        for title_label in f_title_label:\n",
    "            title_label = title_label.split('\\t')\n",
    "            identity = int(title_label[0])\n",
    "            label = title_label[2]\n",
    "            title_id = title_ids_invmap[identity]\n",
    "            label_id = label_to_id[label]\n",
    "            node_labels[title_id] = label_id\n",
    "        node_labels = torch.LongTensor(node_labels)\n",
    "        f_title_label.close()\n",
    "    \n",
    "        \n",
    "        \n",
    "        # Edge data\n",
    "        \n",
    "        ## title-title\n",
    "        title_title_src = []\n",
    "        title_title_dst = []\n",
    "        df_title_title = pd.read_csv(\"../data/cb12/graph/title_title_transition_MinorGroup200_enhanced.csv\", \"\\t\")\n",
    "        for idx, row in df_title_title.iterrows():\n",
    "            title_title_src.append(title_ids_invmap[row['Src']])\n",
    "            title_title_dst.append(title_ids_invmap[row['Dst']])\n",
    "\n",
    "        print('Unique title src: {}'.format(len(set(title_title_src))))\n",
    "        print('Unique title dst: {}'.format(len(set(title_title_dst))))\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "        ## title-tag\n",
    "        title_tag_src = []\n",
    "        title_tag_dst = []\n",
    "        f_title_tag = open(\"../data/cb12/graph/title_tag.txt\", \"r\")\n",
    "        for title_tag in f_title_tag:\n",
    "            title_tag = title_tag.split('\\t')\n",
    "            title = int(title_tag[0])\n",
    "            tag = int(title_tag[1].strip('\\n'))\n",
    "            title_tag_src.append(title_ids_invmap[title])\n",
    "            title_tag_dst.append(tag_ids_invmap[tag])\n",
    "        f_title_tag.close()\n",
    "        print('Unique title: {}'.format(len(set(title_tag_src))))\n",
    "        print('Unique tag: {}'.format(len(set(title_tag_dst))))\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "        heterG = dgl.heterograph({\n",
    "            ('title', 'include', 'tag') : (title_tag_src, title_tag_dst),\n",
    "            ('tag', 'is_included', 'title') : (title_tag_dst, title_tag_src),\n",
    "            ('title', 'tt', 'title') : (title_title_src, title_title_dst),\n",
    "         })\n",
    "        \n",
    "        self.graph = heterG\n",
    "        self.graph.nodes['title'].data['feature'] = node_features\n",
    "        self.graph.nodes['title'].data['label'] = node_labels\n",
    "        \n",
    "        \n",
    "        n_nodes = heterG.number_of_nodes('title')\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        self.graph.nodes['title'].data['train_mask'] = train_mask\n",
    "        self.graph.nodes['title'].data['val_mask'] = val_mask\n",
    "        self.graph.nodes['title'].data['test_mask'] = test_mask\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get graph object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Item index\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`dgl.DGLGraph`\n",
    "            graph structure and labels.\n",
    "            - ``ndata['label']``: ground truth labelsv\n",
    "        \"\"\"\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        return self.graph\n",
    "    \n",
    "    def __len__(self):\n",
    "        r\"\"\"The number of graphs in the dataset.\"\"\"\n",
    "        return 1   \n",
    "    \n",
    "    \n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "   \n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Number of classes.\"\"\"\n",
    "        print(\"Number of classes: {}\".format(len(set(self.all_labels))))\n",
    "        return len(set(self.all_labels))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69153e08-cc8b-4737-8dd9-3dbe00031e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cb12 = CB12Dataset()\n",
    "graph_cb12 = dataset_cb12[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d12457-c5e0-4708-88d3-135c543615cf",
   "metadata": {},
   "source": [
    "# Step 2: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3b885-38a4-4f6d-996a-2a856c21111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805a71b-d564-4609-9a62-06a0fe903c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits, labels):\n",
    "    _, indices = torch.max(logits, dim=1)\n",
    "    preds = indices.long().cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    micro_f1 = f1_score(labels, preds, average='micro')\n",
    "    weighted_f1 = f1_score(labels, preds, average='weighted')\n",
    "    return acc, macro_f1, micro_f1, weighted_f1\n",
    "\n",
    "\n",
    "def evaluate(model, g, features, labels, mask, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "    loss = loss_fn(logits, labels.long())\n",
    "    acc, macro_f1, micro_f1, weighted_f1 = compute_metrics(logits, labels)\n",
    "    return loss, acc, macro_f1, micro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f99f0e-8793-44b6-aa47-d0858dfa2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        dt = datetime.datetime.now()\n",
    "        self.filename = 'early_stop_{}_{:02d}-{:02d}-{:02d}.pth'.format(dt.date(), dt.hour, dt.minute, dt.second)\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def save_checkpoint(self, model, model_name):\n",
    "        \"\"\"\n",
    "        Save model when validation loss decrease\n",
    "        \"\"\"\n",
    "        dirs = os.path.join('../checkpoints/' + model_name)\n",
    "        if not os.path.exists(dirs):\n",
    "            os.makedirs(dirs)\n",
    "        torch.save(model.state_dict(), os.path.join(dirs + '/' + self.filename))\n",
    "    \n",
    "    \n",
    "    def load_checkpoint(self, model, model_name):\n",
    "        \"\"\"\n",
    "        Load the latest checkpoint\n",
    "        \"\"\"\n",
    "        model.load_state_dict(torch.load(os.path.join('../checkpoints/' + model_name + '/' + self.filename)))\n",
    "    \n",
    "                   \n",
    "    \n",
    "    def step(self, model, model_name, loss, acc):\n",
    "        score = acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_loss = loss\n",
    "            self.save_checkpoint(model, model_name)\n",
    "            \n",
    "        elif (loss > self.best_loss) and (acc < self.best_score):\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        else:\n",
    "            if (loss <= self.best_loss) and (acc >= self.best_score):\n",
    "                self.save_checkpoint(model, model_name)\n",
    "                \n",
    "            self.best_score = np.max((acc, self.best_score))\n",
    "            self.best_loss = np.min((loss, self.best_loss))\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "    \n",
    "\n",
    "def train(g, model, model_name, lr, weight_decay, epoch):\n",
    "    stopper = EarlyStopping(patience=100)\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    best_val_macro_f1 = 0\n",
    "    best_test_macro_f1 = 0\n",
    "\n",
    "    features = g.nodes['title'].data['feature']\n",
    "    labels = g.nodes['title'].data['label']\n",
    "    \n",
    "    train_mask = g.nodes['title'].data['train_mask']\n",
    "    val_mask = g.nodes['title'].data['val_mask']\n",
    "    test_mask = g.nodes['title'].data['test_mask']\n",
    "   \n",
    "    \n",
    "    list_train_loss = []\n",
    "    \n",
    "    list_train_acc = []\n",
    "    list_val_acc = []\n",
    "    list_test_acc = []\n",
    "    \n",
    "    list_train_macro_f1 = []\n",
    "    list_val_macro_f1  = []\n",
    "    list_test_macro_f1  = []\n",
    "    \n",
    "    list_train_micro_f1 = []\n",
    "    list_val_micro_f1  = []\n",
    "    list_test_micro_f1  = []\n",
    "    \n",
    "    list_train_weighted_f1 = []\n",
    "    list_val_weighted_f1  = []\n",
    "    list_test_weighted_f1  = []\n",
    "    \n",
    "    \n",
    "    for e in range(epoch):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask].long())\n",
    "        list_train_loss.append(loss.detach().numpy())\n",
    "        \n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc, train_macro_f1, train_micro_f1, train_weighted_f1 = compute_metrics(logits[train_mask], labels[train_mask])\n",
    "        list_train_acc.append(train_acc)\n",
    "        list_train_macro_f1.append(train_macro_f1)\n",
    "        list_train_micro_f1.append(train_micro_f1)\n",
    "        list_train_weighted_f1.append(train_weighted_f1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        val_loss, val_acc, val_macro_f1, val_micro_f1, val_weighted_f1 = evaluate(model, g, features, labels, val_mask, loss_fcn)\n",
    "        list_val_acc.append(val_acc)\n",
    "        list_val_macro_f1.append(val_macro_f1)\n",
    "        list_val_micro_f1.append(val_micro_f1)\n",
    "        list_val_weighted_f1.append(val_weighted_f1)\n",
    "        \n",
    "        if stopper.step(model, model_name, val_loss, val_acc):\n",
    "            break\n",
    "        \n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "\n",
    "        if e % 100 == 0:\n",
    "            print('In epoch {}, loss: {:.4f}'.format(e, loss))\n",
    "            print('train acc: {:.4f}, val acc: {:.4f} (best {:.4f})'.format(train_acc, val_acc, best_val_acc))\n",
    "            print('train macro_f1: {:.4f}, val macro_f1: {:.4f}'.format(train_macro_f1, val_macro_f1))\n",
    "            print('train micro_f1: {:.4f}, val micro_f1: {:.4f}'.format(train_micro_f1, val_micro_f1))\n",
    "            print('train weighted_f1: {:.4f}, val weighted_f1: {:.4f}'.format(train_weighted_f1, val_weighted_f1))\n",
    "            print(\"-----------------------------\")\n",
    "    \n",
    "    stopper.load_checkpoint(model, model_name)\n",
    "    test_loss, test_acc, test_macro_f1, test_micro_f1, test_weighted_f1 = evaluate(model, g, features, labels, test_mask, loss_fcn)\n",
    "    print('test acc: {:.4f}, test macro_f1: {:.4f}, test micro_f1: {:.4f}, test weighted_f1: {:.4f}'.format(test_acc, test_macro_f1, test_micro_f1, test_weighted_f1))\n",
    "            \n",
    "    \n",
    "    \n",
    "    results =  pd.DataFrame({'loss': list_train_loss, \n",
    "                'train_acc': list_train_acc, \n",
    "                'val_acc':list_val_acc,\n",
    "                'train_macro_f1': list_train_macro_f1,\n",
    "                'val_macro_f1': list_val_macro_f1,\n",
    "                'train_micro_f1': list_train_micro_f1,\n",
    "                'val_micro_f1': list_val_micro_f1,\n",
    "                'train_weighted_f1': list_train_weighted_f1,\n",
    "                'val_weighted_f1': list_val_weighted_f1,\n",
    "               })\n",
    "    \n",
    "    dirs = os.path.join('results/' + model_name)\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "            \n",
    "    f_out = open(os.path.join(dirs + '/' + 'lr' + str(lr) +'.pkl'), 'wb')\n",
    "    pickle.dump(results, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fcda4a-4e46-45e3-9d4a-b0f2b752d64c",
   "metadata": {},
   "source": [
    "## HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ce271-d462-4b53-9f2b-2a3bb3f9e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca74d71-fa60-4652-a576-6b8af3265b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticAttention(torch.nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden):\n",
    "        \"\"\"\n",
    "        :param g[dgl]: graph\n",
    "        :param in_feats[int]: dimension of input features\n",
    "        :param n_hidden[int]: number of hidden units\n",
    "        \"\"\"\n",
    "        super(SemanticAttention, self).__init__()\n",
    "        self.project = torch.nn.Sequential(torch.nn.Linear(in_feats, n_hidden), torch.nn.Tanh(), torch.nn.Linear(n_hidden, 1, bias=False))\n",
    "    \n",
    "    def forward(self, z):\n",
    "        w = self.project(z).mean(0) # (N, M, D*K) -> (N, M, 1) -> (M, 1)\n",
    "        beta = torch.softmax(w, dim=0) # (M, 1)\n",
    "        beta = beta.expand((z.shape[0],) + beta.shape) # (N, M, 1)\n",
    "        return (beta * z).sum(1) # (N, M, 1)*(N, M, D*K) = (N, M, D*K) -> sum(1) (N, D*K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7850b-9655-435b-82ad-ef343822b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANLayer(torch.nn.Module):\n",
    "    def __init__(self, meta_paths, in_feats, n_hidden, n_heads, activation, feat_dropout=0.2, attn_dropout=0.2, negative_slope=0.2, residual=False):\n",
    "        \"\"\"\n",
    "        :param meta_paths[list]: list of metapaths, each as a list of edge types\n",
    "        :param in_feats[int]: dimension of input features\n",
    "        :param n_hidden[int]: number of hidden units\n",
    "        :param n_heads[int]: number of hidden attention heads, default=8\n",
    "        :param activation[str]: callable activation function/layer or None, optional.\n",
    "        :param feat_dropout[float]: dropout rate on feature, default=0\n",
    "        :param attn_dropout[float]: dropout rate on attention weight, default=0\n",
    "        :param negative_slope[float]: the negative slope of leaky relu, default=0.2 \n",
    "        :param residual[bool]: use residual connection\n",
    "        \"\"\"\n",
    "        super(HANLayer, self).__init__()\n",
    "        # One GAT layer for each meta path based adjacency matrix\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        for l in range(len(meta_paths)):\n",
    "            self.gat_layers.append(GATConv(in_feats, n_hidden, n_heads, feat_dropout, attn_dropout, negative_slope, residual, activation=F.elu, allow_zero_in_degree=True))\n",
    "        \n",
    "        # Due to multi-head, in_feats=n_hidden*n_heads\n",
    "        self.semantic_attention = SemanticAttention(n_hidden*n_heads, n_hidden)\n",
    "        self.meta_paths = list(tuple(meta_path) for meta_path in meta_paths)\n",
    "        \n",
    "        self._cached_graph = None\n",
    "        self._cached_coalesced_graph = {}\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        semantic_embeddings = []\n",
    "        if self._cached_graph is None or self._cached_graph is not g:\n",
    "            self._cached_graph = g\n",
    "            self._cached_coalesced_graph.clear()\n",
    "            for meta_path in self.meta_paths:\n",
    "                self._cached_coalesced_graph[meta_path] = dgl.metapath_reachable_graph(g, meta_path)\n",
    "        \n",
    "        for i, meta_path in enumerate(self.meta_paths):\n",
    "            new_g = self._cached_coalesced_graph[meta_path] # get homN through metapath\n",
    "            # N: number of nodes\n",
    "            # d: dimension of hidden state\n",
    "            # K: number of attention heads\n",
    "            # M: number of metapaths\n",
    "            semantic_embeddings.append(self.gat_layers[i](new_g, features).flatten(1)) #(N, d*K) \n",
    "        semantic_embeddings = torch.stack(semantic_embeddings, dim=1) # (N, M, d * K)\n",
    "        return self.semantic_attention(semantic_embeddings) # (N, D * K)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31758810-b2d7-4877-b940-32ed74ba35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN(torch.nn.Module):\n",
    "    def __init__(self, meta_paths, in_feats, n_hidden, n_classes, n_heads, n_out_heads, activation, feat_dropout=0.2, attn_dropout=0.2, negative_slope=0.2, residual=False):\n",
    "        \"\"\"\n",
    "        :param meta_paths[list]: list of metapaths, each as a list of edge types\n",
    "        :param in_feats[int]: dimension of input features\n",
    "        :param n_hidden[int]: number of hidden units\n",
    "        :param n_classes[int]: number of classes\n",
    "        :param n_heads[int]: number of hidden attention heads, default=8\n",
    "        :param n_out_heads[int]: number of output attention heads, default=1\n",
    "        :param activation[str]: callable activation function/layer or None, optional.\n",
    "        :param feat_dropout[float]: dropout rate on feature, default=0\n",
    "        :param attn_dropout[float]: dropout rate on attention weight, default=0\n",
    "        :param negative_slope[float]: the negative slope of leaky relu, default=0.2 \n",
    "        :param residual[bool]: use residual connection\n",
    "        \"\"\"\n",
    "        super(HAN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        # Input layer\n",
    "        self.layers.append(HANLayer(meta_paths, in_feats, n_hidden, n_heads[0], activation, feat_dropout, attn_dropout))\n",
    "        \n",
    "        # Hidden layer\n",
    "        for l in range(1, len(n_heads)):\n",
    "            self.layers.append(HANLayer(meta_paths, n_hidden*n_heads[l-1], n_hidden, n_heads[l], activation, feat_dropout, attn_dropout))\n",
    "        \n",
    "        # Output layer\n",
    "        self.predict = torch.nn.Linear(n_hidden*n_heads[-1], n_classes)\n",
    "    \n",
    "    def forward(self, g, featuers):\n",
    "        h = featuers\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = self.predict(h)\n",
    "        return logits\n",
    "    \n",
    "    def embedding(self, g, x, nodes=None):\n",
    "        \"\"\"\n",
    "        Returns the embeddings of the input nodes\n",
    "        Parameters\n",
    "        ----------\n",
    "        nodes: Tensor, optional\n",
    "            Input nodes, if set `None`, will return all the node embedding.\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Node embedding.\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        save(h, 'results/HAN/emb.pkl')\n",
    "        return h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992c87e-f2ac-4005-a76d-ed52c01f1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_HAN = HAN(\n",
    "    meta_paths=[\n",
    "        ['include', 'is_included'],\n",
    "        ['tt']\n",
    "    ],\n",
    "    in_feats=graph_cb12.nodes['title'].data['feature'].shape[1],\n",
    "    n_hidden=16,\n",
    "    n_classes=dataset_cb12.num_classes,\n",
    "    n_heads=[8],\n",
    "    n_out_heads=1,\n",
    "    activation=F.elu,\n",
    "    feat_dropout=0.2,\n",
    "    attn_dropout=0.6,\n",
    "    negative_slope=0.2,\n",
    "    residual=False\n",
    ")\n",
    "train(graph_cb12, model_HAN, 'HAN', lr=0.001, weight_decay=0.0005, epoch=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67761268-5f29-4666-ab50-90a0505c9ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
